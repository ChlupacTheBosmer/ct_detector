{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Installation",
   "id": "6917d5b52343bbaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Install ultralytics if needed (comment out if already installed)\n",
    "# %pip install ultralytics\n",
    "# %pip install pandas\n",
    "# %pip install tabulate   # optional, if you want tabulate"
   ],
   "id": "6cee358f626d89ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Update the settings to point to the datasets and runs directories\n",
    "from ultralytics import settings\n",
    "from ct_detector import DATASETS_DIR, ROOT_DIR\n",
    "import os\n",
    "\n",
    "settings.update({'datasets_dir': DATASETS_DIR, 'runs_dir': os.path.join(ROOT_DIR, 'runs')})"
   ],
   "id": "474735cf69bcd11b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate single model",
   "id": "290b1793a0d0ca7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) evaluate a single model\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from ct_detector.model import evaluate_model\n",
    "from ct_detector.model import MODELS, DATASETS\n",
    "\n",
    "# 2) (Optional) If you have a GPU and want to specify it, set device='0' or 'cpu'\n",
    "model_path = MODELS['eie_t_1_yolov8m']  # or your custom model: 'path/to/best.pt'\n",
    "data_path  = DATASETS.get(\"1\", None)  # or your dataset .yaml or folder of images\n",
    "device = '' # '' or 'cpu for CPU, '0' for GPU 0, '1' for GPU 1, etc.\n",
    "\n",
    "# 4) Run evaluation\n",
    "results = evaluate_model(\n",
    "    model_path=model_path,\n",
    "    data_path=data_path,\n",
    "    conf=0.25,\n",
    "    iou=0.45,\n",
    "    device=device,\n",
    "    imgsz=640,\n",
    "    save_json=False,\n",
    "    project='runs/model_evaluation',\n",
    "    name='demo_evaluate',\n",
    "    verbose=True # True to print results, False to suppress\n",
    ")\n",
    "# 5) (Optional) If you want detailed outputs, you can look at results.keys():\n",
    "print(\"\\nAll results keys:\", dir(results))"
   ],
   "id": "1e09bb650c3d93e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6) (Optional) If you want to display confusion matrix you can do this:\n",
    "from ct_detector.display import display_confusion_matrix\n",
    "from functools import partial\n",
    "\n",
    "callback = partial(display_confusion_matrix, width=640, height=640)\n",
    "results.confusion_matrix.plot(normalize=False, names=tuple(c for c in results.names.values()), on_plot=callback)"
   ],
   "id": "1309116f07d5da98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate multiple models",
   "id": "5af5d5d5f76d5c94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2) compare models\n",
    "from ct_detector.model import compare_models\n",
    "from ct_detector.model import MODELS, DATASETS\n",
    "\n",
    "# Print all available models\n",
    "print(\"Available models:\")\n",
    "for model_name, model_path in MODELS.items():\n",
    "    print(f\"{model_name}: {model_path}\")\n",
    "\n",
    "# Print all available datasets\n",
    "print(\"\\nAvailable datasets:\")\n",
    "for dataset_name, dataset_path in DATASETS.items():\n",
    "    print(f\"{dataset_name}: {dataset_path}\")\n",
    "\n",
    "# Create a list of model paths to compare from the MODELS dictionary that automatically gets all available models\n",
    "model_list = [path for path in MODELS.values() if path.endswith('.pt')]\n",
    "#model_list = [MODELS['eie_t_1_yolov8m']]  # Add more models as needed\n",
    "\n",
    "# Alternatively, you can specify specific models like this:\n",
    "# model_list = [MODELS['yolov8n.pt'], MODELS['yolov8s.pt']]  # Add more models as needed\n",
    "\n",
    "# Get the default dataset path from the DATASETS dictionary which is a dictionary of available datasets\n",
    "data_path = DATASETS.get(\"1\", None)  # Or 'path/to/yourData.yaml'\n",
    "\n",
    "results = compare_models(\n",
    "    model_paths=model_list,\n",
    "    data_path=data_path,\n",
    "    conf=0.25,\n",
    "    iou=0.45,\n",
    "    device='',   # GPU if available, else '' or 'cpu'\n",
    "    imgsz=640,\n",
    "    project='runs/model_comparison',\n",
    "    name='demo_compare',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# results is a dictionary of results for each model where keys are model names and values are the results\n",
    "print(\"\\n--- Retrieved Evaluation Results For: ---\")\n",
    "for key in results.keys():\n",
    "    print(f\"\\n--- {key} ---\")"
   ],
   "id": "df16ad848c7afc55",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
